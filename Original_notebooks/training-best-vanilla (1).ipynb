{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport wandb\nfrom tqdm.notebook import tqdm\nimport random\nimport matplotlib.pyplot as plt\nimport copy\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n# Download data (if needed)\n!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n!tar -xf dakshina_dataset_v1.0.tar\n\n# Define paths\ntrain_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\nval_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\ntest_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n\n# Load data\ntrain_df = pd.read_csv(train_path, delimiter='\\t', names=['hi', 'en', '_'])\nval_df = pd.read_csv(val_path, delimiter='\\t', names=['hi', 'en', '_'])\ntest_df = pd.read_csv(test_path, delimiter='\\t', names=['hi', 'en', '_'])\n\nprint(f\"Train samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\nprint(f\"Test samples: {len(test_df)}\")\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:48:10.530381Z","iopub.execute_input":"2025-05-20T16:48:10.530594Z","iopub.status.idle":"2025-05-20T16:49:25.088563Z","shell.execute_reply.started":"2025-05-20T16:48:10.530576Z","shell.execute_reply":"2025-05-20T16:49:25.087574Z"}},"outputs":[{"name":"stdout","text":"--2025-05-20 16:48:18--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.174.207, 74.125.203.207, 108.177.97.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.174.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2008340480 (1.9G) [application/x-tar]\nSaving to: ‘dakshina_dataset_v1.0.tar’\n\ndakshina_dataset_v1 100%[===================>]   1.87G  30.9MB/s    in 64s     \n\n2025-05-20 16:49:22 (30.1 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n\nTrain samples: 44204\nValidation samples: 4358\nTest samples: 4502\nUsing device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Create character mappings with safety checks\ndef create_vocab(texts, special_tokens=True):\n    chars = set()\n    for text in texts:\n        for char in str(text):\n            chars.add(char)\n    \n    # Create vocabulary dictionary\n    if special_tokens:\n        vocab = {'<PAD>': 0, '< SOS >': 1, '<EOS>': 2, '<UNK>': 3}\n    else:\n        vocab = {}\n        \n    for i, char in enumerate(sorted(list(chars))):\n        vocab[char] = i + 4\n    \n    return vocab\n\n# Convert text to indices with better error handling\ndef text_to_indices(text, vocab):\n    indices = [vocab['< SOS >']]\n    for char in str(text):\n        if char in vocab:\n            indices.append(vocab[char])\n        elif char.lower() in vocab:\n            indices.append(vocab[char.lower()])\n        else:\n            indices.append(vocab['<UNK>'])\n    indices.append(vocab['<EOS>'])\n    return indices\n\n# Create vocabularies\nsrc_vocab = create_vocab(train_df['en'])\ntgt_vocab = create_vocab(train_df['hi'])\n\n# Create reverse mappings for visualization\nidx2src = {idx: char for char, idx in src_vocab.items()}\nidx2tgt = {idx: char for char, idx in tgt_vocab.items()}\n\nprint(f\"Source vocabulary size: {len(src_vocab)}\")\nprint(f\"Target vocabulary size: {len(tgt_vocab)}\")\n\n# Check max sequence lengths\nsrc_max_len = max([len(str(text)) for text in train_df['en']])\ntgt_max_len = max([len(str(text)) for text in train_df['hi']])\nprint(f\"Max source sequence length: {src_max_len}\")\nprint(f\"Max target sequence length: {tgt_max_len}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:49:25.090314Z","iopub.execute_input":"2025-05-20T16:49:25.090651Z","iopub.status.idle":"2025-05-20T16:49:25.156399Z","shell.execute_reply.started":"2025-05-20T16:49:25.090627Z","shell.execute_reply":"2025-05-20T16:49:25.155684Z"}},"outputs":[{"name":"stdout","text":"Source vocabulary size: 30\nTarget vocabulary size: 67\nMax source sequence length: 20\nMax target sequence length: 19\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Custom Dataset\nclass TransliterationDataset(Dataset):\n    def __init__(self, dataframe, src_vocab, tgt_vocab):\n        self.dataframe = dataframe\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        src_text = self.dataframe.iloc[idx]['en']\n        tgt_text = self.dataframe.iloc[idx]['hi']\n        \n        src_indices = text_to_indices(src_text, self.src_vocab)\n        tgt_indices = text_to_indices(tgt_text, self.tgt_vocab)\n        \n        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n\n# Collate function with safety checks\ndef collate_fn(batch):\n    src_batch, tgt_batch = [], []\n    for src, tgt in batch:\n        # Safety check for index bounds\n        src = torch.clamp(src, 0, len(src_vocab)-1)\n        tgt = torch.clamp(tgt, 0, len(tgt_vocab)-1)\n        \n        src_batch.append(src)\n        tgt_batch.append(tgt)\n    \n    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab['<PAD>'])\n    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab['<PAD>'])\n    \n    return src_batch, tgt_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:49:25.157108Z","iopub.execute_input":"2025-05-20T16:49:25.157353Z","iopub.status.idle":"2025-05-20T16:49:25.163758Z","shell.execute_reply.started":"2025-05-20T16:49:25.157335Z","shell.execute_reply":"2025-05-20T16:49:25.163181Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, num_layers, dropout, cell_type):\n        super().__init__()\n        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n        self.cell_type = cell_type.lower()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        if self.cell_type == \"lstm\":\n            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n                              dropout=dropout if num_layers > 1 else 0, batch_first=True)\n        elif self.cell_type == \"gru\":\n            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, \n                             dropout=dropout if num_layers > 1 else 0, batch_first=True)\n        else:  # rnn\n            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, \n                             dropout=dropout if num_layers > 1 else 0, batch_first=True)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src):\n        # src: [batch_size, src_len]\n        embedded = self.dropout(self.embedding(src))  # [batch_size, src_len, emb_dim]\n        \n        if self.cell_type == \"lstm\":\n            outputs, (hidden, cell) = self.rnn(embedded)\n            return outputs, hidden, cell\n        else:\n            outputs, hidden = self.rnn(embedded)\n            return outputs, hidden, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:49:25.165416Z","iopub.execute_input":"2025-05-20T16:49:25.165820Z","iopub.status.idle":"2025-05-20T16:49:25.184966Z","shell.execute_reply.started":"2025-05-20T16:49:25.165799Z","shell.execute_reply":"2025-05-20T16:49:25.184244Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_vocab_size, embedding_dim, hidden_dim, num_layers, dropout, cell_type):\n        super().__init__()\n        self.output_vocab_size = output_vocab_size\n        self.embedding = nn.Embedding(output_vocab_size, embedding_dim)\n        self.cell_type = cell_type.lower()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        if self.cell_type == \"lstm\":\n            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n                              dropout=dropout if num_layers > 1 else 0, batch_first=True)\n        elif self.cell_type == \"gru\":\n            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, \n                             dropout=dropout if num_layers > 1 else 0, batch_first=True)\n        else:  # rnn\n            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, \n                             dropout=dropout if num_layers > 1 else 0, batch_first=True)\n        \n        self.fc_out = nn.Linear(hidden_dim, output_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, cell=None):\n        # input: [batch_size]\n        input = input.unsqueeze(1)  # [batch_size, 1]\n        embedded = self.dropout(self.embedding(input))  # [batch_size, 1, emb_dim]\n        \n        if self.cell_type == \"lstm\":\n            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        else:\n            output, hidden = self.rnn(embedded, hidden)\n            cell = None\n            \n        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_vocab_size]\n        \n        return prediction, hidden, cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:49:25.185825Z","iopub.execute_input":"2025-05-20T16:49:25.186078Z","iopub.status.idle":"2025-05-20T16:49:25.203599Z","shell.execute_reply.started":"2025-05-20T16:49:25.186053Z","shell.execute_reply":"2025-05-20T16:49:25.203019Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = Encoder(\n            config['input_vocab_size'],\n            config['embedding_dim'],\n            config['hidden_dim'],\n            config['num_encoding_layers'],\n            config['dropout'],\n            config['cell_type']\n        )\n        self.decoder = Decoder(\n            config['output_vocab_size'],\n            config['embedding_dim'],\n            config['hidden_dim'],\n            config['num_decoding_layers'],\n            config['dropout'],\n            config['cell_type']\n        )\n        self.device = config.get('device', device)\n        self.teacher_forcing_ratio = config.get('teacher_forcing_ratio', 0.5)\n        self.cell_type = config['cell_type'].lower()\n        self.config = config\n        \n        # Apply weight initialization\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        if isinstance(m, nn.GRU) or isinstance(m, nn.LSTM) or isinstance(m, nn.RNN):\n            for name, param in m.named_parameters():\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(param.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(param.data)  # Orthogonal initialization for recurrent weights\n                elif 'bias' in name:\n                    param.data.fill_(0)\n        elif isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0)\n        elif isinstance(m, nn.Embedding):\n            nn.init.uniform_(m.weight.data, -0.1, 0.1)\n        \n    def forward(self, src, trg):\n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = self.decoder.output_vocab_size\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        \n        # Encode source sequence\n        encoder_outputs, hidden, cell = self.encoder(src)\n        \n        # Adjust hidden state dimensions if needed\n        enc_layers = self.config['num_encoding_layers']\n        dec_layers = self.config['num_decoding_layers']\n        hidden_size = self.config['hidden_dim']\n        \n        if enc_layers != dec_layers:\n            if self.cell_type != 'lstm':\n                # Case 1: Encoder has more layers - take only what we need\n                if enc_layers > dec_layers:\n                    hidden = hidden[:dec_layers]\n                # Case 2: Decoder has more layers - pad with zeros\n                else:\n                    padding = torch.zeros(dec_layers - enc_layers, batch_size, hidden_size).to(self.device)\n                    hidden = torch.cat([hidden, padding], dim=0)\n            else:  # LSTM case\n                if enc_layers > dec_layers:\n                    hidden = hidden[:dec_layers]\n                    cell = cell[:dec_layers]\n                else:\n                    padding = torch.zeros(dec_layers - enc_layers, batch_size, hidden_size).to(self.device)\n                    hidden = torch.cat([hidden, padding], dim=0)\n                    cell = torch.cat([cell, padding], dim=0)\n        \n        # First input to decoder is < SOS > token\n        input = trg[:, 0]\n        \n        for t in range(1, trg_len):\n            # Get decoder output\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            \n            # Store prediction\n            outputs[:, t, :] = output\n            \n            # Teacher forcing\n            teacher_force = random.random() < self.teacher_forcing_ratio\n            \n            # Get highest predicted token\n            top1 = output.argmax(1)\n            \n            # Next input is either ground truth or predicted token\n            input = trg[:, t] if teacher_force else top1\n            \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:49:25.206545Z","iopub.execute_input":"2025-05-20T16:49:25.206851Z","iopub.status.idle":"2025-05-20T16:49:25.226375Z","shell.execute_reply.started":"2025-05-20T16:49:25.206824Z","shell.execute_reply":"2025-05-20T16:49:25.225706Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, criterion, clip=1.0):\n    model.train()\n    epoch_loss = 0\n    \n    for src, trg in tqdm(dataloader, desc=\"Training\", leave=False):\n        src, trg = src.to(model.device), trg.to(model.device)\n        \n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        # Exclude < SOS > token\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        trg = trg[:, 1:].reshape(-1)\n        \n        loss = criterion(output, trg)\n        loss.backward()\n        \n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    return epoch_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:49:25.227114Z","iopub.execute_input":"2025-05-20T16:49:25.227470Z","iopub.status.idle":"2025-05-20T16:49:25.242768Z","shell.execute_reply.started":"2025-05-20T16:49:25.227441Z","shell.execute_reply":"2025-05-20T16:49:25.241964Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion):\n    model.eval()\n    epoch_loss = 0\n    \n    # For exact match accuracy\n    exact_match_correct = 0\n    exact_match_total = 0\n    \n    # For character-level accuracy\n    char_correct = 0\n    char_total = 0\n    \n    with torch.no_grad():\n        for src, trg in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            src, trg = src.to(model.device), trg.to(model.device)\n            \n            output = model(src, trg)\n            \n            # For loss calculation\n            output_dim = output.shape[-1]\n            output_flat = output[:, 1:].reshape(-1, output_dim)\n            trg_flat = trg[:, 1:].reshape(-1)\n            \n            loss = criterion(output_flat, trg_flat)\n            epoch_loss += loss.item()\n            \n            # Get predictions\n            predictions = output.argmax(dim=2)\n            \n            # Calculate exact match accuracy (sequence level)\n            for i in range(len(predictions)):\n                pred_seq = predictions[i, 1:].cpu().numpy()  # Skip < SOS >\n                target_seq = trg[i, 1:].cpu().numpy()  # Skip < SOS >\n                \n                # Get valid sequence (remove padding)\n                valid_length = (target_seq != tgt_vocab['<PAD>']).sum()\n                pred_clean = pred_seq[:valid_length]\n                target_clean = target_seq[:valid_length]\n                \n                # Check exact match\n                if np.array_equal(pred_clean, target_clean):\n                    exact_match_correct += 1\n                exact_match_total += 1\n                \n                # Calculate character-level accuracy\n                for j in range(valid_length):\n                    if pred_seq[j] == target_seq[j]:\n                        char_correct += 1\n                    char_total += 1\n    \n    # Calculate metrics\n    exact_match_accuracy = exact_match_correct / exact_match_total if exact_match_total > 0 else 0\n    char_accuracy = char_correct / char_total if char_total > 0 else 0\n    \n    return {\n        'loss': epoch_loss / len(dataloader),\n        'exact_match_accuracy': exact_match_accuracy,\n        'char_accuracy': char_accuracy\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:49:25.243682Z","iopub.execute_input":"2025-05-20T16:49:25.243952Z","iopub.status.idle":"2025-05-20T16:49:25.260403Z","shell.execute_reply.started":"2025-05-20T16:49:25.243926Z","shell.execute_reply":"2025-05-20T16:49:25.259763Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def translate_sentence(model, sentence, src_vocab, tgt_vocab, idx2tgt, max_len=50):\n    model.eval()\n    \n    # Convert to indices and add < SOS > and <EOS>\n    indices = text_to_indices(sentence, src_vocab)\n    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(model.device)\n    \n    # Get encoder outputs\n    with torch.no_grad():\n        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n    \n    # Adjust hidden state dimensions if needed\n    enc_layers = model.config['num_encoding_layers']\n    dec_layers = model.config['num_decoding_layers']\n    hidden_size = model.config['hidden_dim']\n    \n    if enc_layers != dec_layers:\n        batch_size = 1  # Since we're translating one sentence\n        if model.cell_type != 'lstm':\n            if enc_layers > dec_layers:\n                hidden = hidden[:dec_layers]\n            else:\n                padding = torch.zeros(dec_layers - enc_layers, batch_size, hidden_size).to(model.device)\n                hidden = torch.cat([hidden, padding], dim=0)\n        else:  # LSTM case\n            if enc_layers > dec_layers:\n                hidden = hidden[:dec_layers]\n                cell = cell[:dec_layers]\n            else:\n                padding = torch.zeros(dec_layers - enc_layers, batch_size, hidden_size).to(model.device)\n                hidden = torch.cat([hidden, padding], dim=0)\n                cell = torch.cat([cell, padding], dim=0)\n    \n    # Start with < SOS > token\n    trg_idx = [tgt_vocab['< SOS >']]\n    \n    for _ in range(max_len):\n        trg_tensor = torch.LongTensor([trg_idx[-1]]).to(model.device)\n        \n        with torch.no_grad():\n            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n        \n        pred_token = output.argmax(1).item()\n        \n        # Stop if <EOS> token\n        if pred_token == tgt_vocab['<EOS>']:\n            break\n        \n        trg_idx.append(pred_token)\n    \n    # Convert indices to characters\n    trg_tokens = [idx2tgt[i] for i in trg_idx if i not in [tgt_vocab['< SOS >'], tgt_vocab['<EOS>'], tgt_vocab['<PAD>']]]\n    \n    return ''.join(trg_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:49:25.261969Z","iopub.execute_input":"2025-05-20T16:49:25.262232Z","iopub.status.idle":"2025-05-20T16:49:25.278326Z","shell.execute_reply.started":"2025-05-20T16:49:25.262214Z","shell.execute_reply":"2025-05-20T16:49:25.277654Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!pip install wandb -q\n\nimport wandb\nwandb.login(key='130161b8988911058327a18dbbdfb663c58411b2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:49:25.279134Z","iopub.execute_input":"2025-05-20T16:49:25.279463Z","iopub.status.idle":"2025-05-20T16:49:36.491526Z","shell.execute_reply.started":"2025-05-20T16:49:25.279409Z","shell.execute_reply":"2025-05-20T16:49:36.490935Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m005\u001b[0m (\u001b[33mda24m005-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Early stopping setup\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0.0):\n        \"\"\"\n        Args:\n            patience: Number of epochs to wait if no improvement and then stop training\n            min_delta: Minimum change in monitored value to qualify as improvement\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        score = -val_loss  # Higher score is better (i.e., lower validation loss)\n        \n        if self.best_score is None:\n            self.best_score = score\n        elif score < self.best_score + self.min_delta:  # No improvement\n            self.counter += 1\n            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:  # Improvement\n            self.best_score = score\n            self.counter = 0\n            \n        return self.early_stop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:53:46.877118Z","iopub.execute_input":"2025-05-20T16:53:46.878129Z","iopub.status.idle":"2025-05-20T16:53:46.883687Z","shell.execute_reply.started":"2025-05-20T16:53:46.878099Z","shell.execute_reply":"2025-05-20T16:53:46.882871Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train_best_model(config, epochs=40):\n    # Initialize wandb run for final training\n    run = wandb.init(project=\"seq2seq-transliteration-final\", name=\"best_model_final_training\")\n    \n    # Log the configuration\n    wandb.config.update(config)\n    \n    # Create datasets\n    train_dataset = TransliterationDataset(train_df, src_vocab, tgt_vocab)\n    val_dataset = TransliterationDataset(val_df, src_vocab, tgt_vocab)\n    test_dataset = TransliterationDataset(test_df, src_vocab, tgt_vocab)\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        collate_fn=collate_fn\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n    \n    # Initialize model with weight initialization\n    model = Seq2Seq(config).to(device)\n    \n    # Apply weight initialization\n    def init_weights(m):\n        if isinstance(m, nn.GRU) or isinstance(m, nn.LSTM) or isinstance(m, nn.RNN):\n            for name, param in m.named_parameters():\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(param.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(param.data)\n                elif 'bias' in name:\n                    param.data.fill_(0)\n        elif isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0)\n        elif isinstance(m, nn.Embedding):\n            nn.init.uniform_(m.weight.data, -0.1, 0.1)\n    \n    model.apply(init_weights)\n    \n    # Optimizer and criterion\n    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<PAD>'])\n    \n    # Initialize early stopping\n    early_stopping = EarlyStopping(patience=10)\n    \n    # Training loop\n    best_exact_match = 0\n    best_model_state = None\n    \n    for epoch in range(epochs):\n        # Train\n        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n        \n        # Evaluate on validation set\n        eval_metrics = evaluate(model, val_loader, criterion)\n        \n        # Log metrics\n        wandb.log({\n            'epoch': epoch,\n            'train_loss': train_loss,\n            'val_loss': eval_metrics['loss'],\n            'val_exact_match': eval_metrics['exact_match_accuracy'],\n            'val_char_accuracy': eval_metrics['char_accuracy']\n        })\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {eval_metrics['loss']:.4f}\")\n        print(f\"Exact Match Accuracy: {eval_metrics['exact_match_accuracy']:.4f}, Char Accuracy: {eval_metrics['char_accuracy']:.4f}\")\n        \n        # Save best model\n        if eval_metrics['exact_match_accuracy'] > best_exact_match:\n            best_exact_match = eval_metrics['exact_match_accuracy']\n            best_model_state = model.state_dict().copy()\n            torch.save(model.state_dict(), 'best_model_final.pt')\n            wandb.save('best_model_final.pt')\n        \n        # Check early stopping condition\n        if early_stopping(eval_metrics['loss']):\n            print(\"Early stopping triggered!\")\n            break\n    \n    # Load best model for testing\n    model.load_state_dict(best_model_state)\n    \n    # Evaluate on test set\n    test_metrics = evaluate(model, test_loader, criterion)\n    \n    # Log final test metrics\n    wandb.run.summary['test_loss'] = test_metrics['loss']\n    wandb.run.summary['test_exact_match'] = test_metrics['exact_match_accuracy']\n    wandb.run.summary['test_char_accuracy'] = test_metrics['char_accuracy']\n    \n    print(f\"\\nFinal Test Results:\")\n    print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n    print(f\"Test Exact Match Accuracy: {test_metrics['exact_match_accuracy']:.4f}\")\n    print(f\"Test Character Accuracy: {test_metrics['char_accuracy']:.4f}\")\n    \n    # Generate predictions for test examples\n    predictions = []\n    for i in range(len(test_df)):\n        src_text = test_df.iloc[i]['en']\n        tgt_text = test_df.iloc[i]['hi']\n        pred_text = translate_sentence(model, src_text, src_vocab, tgt_vocab, idx2tgt)\n        \n        predictions.append({\n            'Source': src_text,\n            'Target': tgt_text,\n            'Prediction': pred_text,\n            'Correct': pred_text == tgt_text\n        })\n    \n    # Create a table for predictions\n    prediction_table = wandb.Table(dataframe=pd.DataFrame(predictions))\n    wandb.log({\"test_predictions\": prediction_table})\n    \n    # Save predictions to file for assignment submission\n    pd.DataFrame(predictions).to_csv('predictions_vanilla.csv', index=False)\n    wandb.save('predictions_vanilla.csv')\n    \n    # Create error analysis\n    error_examples = [p for p in predictions if not p['Correct']]\n    error_table = wandb.Table(dataframe=pd.DataFrame(error_examples[:20]))\n    wandb.log({\"error_analysis\": error_table})\n    \n    # Close wandb run\n    wandb.finish()\n    \n    return model, pd.DataFrame(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:53:47.856923Z","iopub.execute_input":"2025-05-20T16:53:47.857644Z","iopub.status.idle":"2025-05-20T16:53:47.871369Z","shell.execute_reply.started":"2025-05-20T16:53:47.857616Z","shell.execute_reply":"2025-05-20T16:53:47.870574Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def get_best_config(project_name, entity=\"da24m005-iit-madras\"):\n    \"\"\"\n    Retrieves the best configuration from a wandb sweep based on validation accuracy.\n    \n    Args:\n        project_name (str): The name of your wandb project\n        entity (str, optional): Your wandb username/entity\n    \n    Returns:\n        dict: The configuration of the best performing run\n    \"\"\"\n    api = wandb.Api()\n    path = f\"{entity}/{project_name}\" if entity else project_name\n    runs = api.runs(path)\n    \n    best_run = None\n    best_metric = float('-inf')\n    \n    for run in runs:\n        if run.state == \"finished\":\n            # Check different possible metric names\n            val_metric = (run.summary.get('val_exact_match') or \n                         run.summary.get('val_accuracy') or \n                         run.summary.get('best_val_accuracy'))\n            \n            if val_metric is not None and val_metric > best_metric:\n                best_metric = val_metric\n                best_run = run\n    \n    if best_run:\n        print(f\"Best run ID: {best_run.id}\")\n        print(f\"Best run name: {best_run.name}\")\n        print(f\"Best validation metric: {best_metric:.4f}\")\n        \n        # Create a clean config dictionary\n        best_config = {\n            'input_vocab_size': len(src_vocab),\n            'output_vocab_size': len(tgt_vocab),\n            'embedding_dim': best_run.config['embedding_dim'],\n            'hidden_dim': best_run.config['hidden_dim'],\n            'num_encoding_layers': best_run.config['num_encoding_layers'],\n            'num_decoding_layers': best_run.config['num_decoding_layers'],\n            'dropout': best_run.config['dropout'],\n            'cell_type': best_run.config['cell_type'],\n            'teacher_forcing_ratio': best_run.config['teacher_forcing_ratio'],\n            'learning_rate': best_run.config['learning_rate'],\n            'batch_size': best_run.config['batch_size'],\n            'device': device\n        }\n        \n        print(f\"Best config: {best_config}\")\n        return best_config\n    else:\n        print(\"No finished runs found.\")\n        return None\n\nbest_config = get_best_config('seq2seq-transliteration')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:54:20.997311Z","iopub.execute_input":"2025-05-20T16:54:20.997895Z","iopub.status.idle":"2025-05-20T16:54:35.100052Z","shell.execute_reply.started":"2025-05-20T16:54:20.997871Z","shell.execute_reply":"2025-05-20T16:54:35.099276Z"}},"outputs":[{"name":"stdout","text":"Best run ID: zanpt1wh\nBest run name: trim-sweep-8\nBest validation metric: 0.4190\nBest config: {'input_vocab_size': 30, 'output_vocab_size': 67, 'embedding_dim': 512, 'hidden_dim': 256, 'num_encoding_layers': 2, 'num_decoding_layers': 3, 'dropout': 0.3, 'cell_type': 'lstm', 'teacher_forcing_ratio': 0.9, 'learning_rate': 0.0006954403380443931, 'batch_size': 128, 'device': device(type='cuda')}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Train the best model for 40 epochs\nbest_model, predictions = train_best_model(best_config, epochs=40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:55:22.216217Z","iopub.execute_input":"2025-05-20T16:55:22.216560Z","iopub.status.idle":"2025-05-20T17:04:52.047268Z","shell.execute_reply.started":"2025-05-20T16:55:22.216536Z","shell.execute_reply":"2025-05-20T17:04:52.046415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_165522-4twjet2o</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final/runs/4twjet2o' target=\"_blank\">best_model_final_training</a></strong> to <a href='https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final' target=\"_blank\">https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final/runs/4twjet2o' target=\"_blank\">https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final/runs/4twjet2o</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/40, Train Loss: 3.0708, Val Loss: 2.7767\nExact Match Accuracy: 0.0000, Char Accuracy: 0.2610\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/40, Train Loss: 2.4477, Val Loss: 2.1535\nExact Match Accuracy: 0.0009, Char Accuracy: 0.3920\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/40, Train Loss: 1.8246, Val Loss: 1.4178\nExact Match Accuracy: 0.0425, Char Accuracy: 0.5928\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/40, Train Loss: 1.1829, Val Loss: 0.9346\nExact Match Accuracy: 0.1613, Char Accuracy: 0.7262\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/40, Train Loss: 0.8396, Val Loss: 0.7208\nExact Match Accuracy: 0.2565, Char Accuracy: 0.7891\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/40, Train Loss: 0.6873, Val Loss: 0.6312\nExact Match Accuracy: 0.2969, Char Accuracy: 0.8119\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/40, Train Loss: 0.5810, Val Loss: 0.5969\nExact Match Accuracy: 0.3346, Char Accuracy: 0.8238\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/40, Train Loss: 0.5225, Val Loss: 0.5278\nExact Match Accuracy: 0.3522, Char Accuracy: 0.8396\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/40, Train Loss: 0.4675, Val Loss: 0.5016\nExact Match Accuracy: 0.3690, Char Accuracy: 0.8495\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/40, Train Loss: 0.4357, Val Loss: 0.5094\nExact Match Accuracy: 0.3825, Char Accuracy: 0.8470\nEarlyStopping counter: 1 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/40, Train Loss: 0.3944, Val Loss: 0.5023\nExact Match Accuracy: 0.3873, Char Accuracy: 0.8511\nEarlyStopping counter: 2 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12/40, Train Loss: 0.3697, Val Loss: 0.5028\nExact Match Accuracy: 0.3885, Char Accuracy: 0.8509\nEarlyStopping counter: 3 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13/40, Train Loss: 0.3374, Val Loss: 0.4763\nExact Match Accuracy: 0.3915, Char Accuracy: 0.8572\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14/40, Train Loss: 0.3225, Val Loss: 0.4862\nExact Match Accuracy: 0.4009, Char Accuracy: 0.8572\nEarlyStopping counter: 1 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15/40, Train Loss: 0.2959, Val Loss: 0.4953\nExact Match Accuracy: 0.4006, Char Accuracy: 0.8568\nEarlyStopping counter: 2 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa536b6e5ab244c98f03b6a1f44e1bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0398b7ea69646ada1979936f30b43cd"}},"metadata":{}},{"name":"stdout","text":"Epoch 16/40, Train Loss: 0.2808, Val Loss: 0.5078\nExact Match Accuracy: 0.3988, Char Accuracy: 0.8572\nEarlyStopping counter: 3 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2212e0d0637e4843bee290d35aedce9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746491d234a54ba1a6b91a0f08efcc62"}},"metadata":{}},{"name":"stdout","text":"Epoch 17/40, Train Loss: 0.2669, Val Loss: 0.4511\nExact Match Accuracy: 0.4013, Char Accuracy: 0.8657\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18/40, Train Loss: 0.2480, Val Loss: 0.4643\nExact Match Accuracy: 0.4130, Char Accuracy: 0.8669\nEarlyStopping counter: 1 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19/40, Train Loss: 0.2336, Val Loss: 0.5135\nExact Match Accuracy: 0.4167, Char Accuracy: 0.8601\nEarlyStopping counter: 2 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20/40, Train Loss: 0.2242, Val Loss: 0.4946\nExact Match Accuracy: 0.4080, Char Accuracy: 0.8621\nEarlyStopping counter: 3 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 21/40, Train Loss: 0.2119, Val Loss: 0.5359\nExact Match Accuracy: 0.4114, Char Accuracy: 0.8587\nEarlyStopping counter: 4 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 22/40, Train Loss: 0.2018, Val Loss: 0.5139\nExact Match Accuracy: 0.4066, Char Accuracy: 0.8619\nEarlyStopping counter: 5 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 23/40, Train Loss: 0.1937, Val Loss: 0.5237\nExact Match Accuracy: 0.4101, Char Accuracy: 0.8595\nEarlyStopping counter: 6 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 24/40, Train Loss: 0.1863, Val Loss: 0.4839\nExact Match Accuracy: 0.4087, Char Accuracy: 0.8669\nEarlyStopping counter: 7 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 25/40, Train Loss: 0.1781, Val Loss: 0.5435\nExact Match Accuracy: 0.4121, Char Accuracy: 0.8577\nEarlyStopping counter: 8 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 26/40, Train Loss: 0.1719, Val Loss: 0.4950\nExact Match Accuracy: 0.4133, Char Accuracy: 0.8662\nEarlyStopping counter: 9 out of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 27/40, Train Loss: 0.1620, Val Loss: 0.5338\nExact Match Accuracy: 0.4045, Char Accuracy: 0.8617\nEarlyStopping counter: 10 out of 10\nEarly stopping triggered!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nFinal Test Results:\nTest Loss: 0.5370\nTest Exact Match Accuracy: 0.4096\nTest Character Accuracy: 0.8609\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▇▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_char_accuracy</td><td>▁▃▅▆▇▇█████████████████████</td></tr><tr><td>val_exact_match</td><td>▁▁▂▄▅▆▇▇▇▇█████████████████</td></tr><tr><td>val_loss</td><td>█▆▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>test_char_accuracy</td><td>0.86092</td></tr><tr><td>test_exact_match</td><td>0.4096</td></tr><tr><td>test_loss</td><td>0.53696</td></tr><tr><td>train_loss</td><td>0.16198</td></tr><tr><td>val_char_accuracy</td><td>0.86169</td></tr><tr><td>val_exact_match</td><td>0.40454</td></tr><tr><td>val_loss</td><td>0.53378</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">best_model_final_training</strong> at: <a href='https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final/runs/4twjet2o' target=\"_blank\">https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final/runs/4twjet2o</a><br> View project at: <a href='https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final' target=\"_blank\">https://wandb.ai/da24m005-iit-madras/seq2seq-transliteration-final</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_165522-4twjet2o/logs</code>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"predictions.to_csv('predictions_vanilla.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"hello\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:05:05.103540Z","iopub.execute_input":"2025-05-20T17:05:05.104181Z","iopub.status.idle":"2025-05-20T17:05:05.108312Z","shell.execute_reply.started":"2025-05-20T17:05:05.104155Z","shell.execute_reply":"2025-05-20T17:05:05.107542Z"}},"outputs":[{"name":"stdout","text":"hello\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"predictions.to_csv('preds.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:07:00.404366Z","iopub.execute_input":"2025-05-20T17:07:00.404987Z","iopub.status.idle":"2025-05-20T17:07:00.419631Z","shell.execute_reply.started":"2025-05-20T17:07:00.404959Z","shell.execute_reply":"2025-05-20T17:07:00.419060Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}